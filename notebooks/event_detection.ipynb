{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pylab \n",
    "\n",
    "import spacy\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import TSNE \n",
    "from sklearn.metrics import pairwise_distances_argmin_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('../data/nyt_data_20200202.csv')\n",
    "\n",
    "df = pd.read_csv('../data/news_api.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['publishedAt'] = pd.to_datetime(df['publishedAt'])\n",
    "\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give meaning to independent words and, consequently, whole sentences, we’ll use SpaCy’s pre-trained word embeddings models. More specifically, SpaCy’s large model (en_core_web_lg), which has pre-trained word vectors for 685k English words. Alternatively, you could be using any pre-trained word representation model (Word2Vec, FastText, GloVe…).\n",
    "\n",
    "By default, SpaCy considers a sentence’s vector as the average between every word’s vector. It’s a simplistic approach that doesn’t take into account the order of words to determine a sentence’s vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vecs = {}\n",
    "docs = []\n",
    "\n",
    "for title in tqdm_notebook(df.title):\n",
    "    doc = nlp(title)\n",
    "    docs.append(doc)\n",
    "    sent_vecs.update({title: doc.vector})\n",
    "\n",
    "sentences = list(sent_vecs.keys())\n",
    "vectors = list(sent_vecs.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The epsilon parameter determines the maximum distance between two samples for them to be considered as in the same neighborhood, meaning that if eps is too big, fewer clusters will be formed, but also if it’s too small, most of the points will be classified as not belonging to a cluster (-1), which will result in a few clusters as well.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(vectors)\n",
    "\n",
    "n_classes = {}\n",
    "\n",
    "for i in tqdm_notebook(np.arange(0.001, 1, 0.001)):\n",
    "    dbscan = DBSCAN(eps=i,\n",
    "                    min_samples=2,\n",
    "                    metric='cosine',\n",
    "                    n_jobs=-1).fit(x)\n",
    "    n_classes.update({i: len(pd.Series(dbscan.labels_).value_counts())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(n_classes.keys()), list(n_classes.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tunning eps value might be one of the most delicate steps because the outcome will vary a lot depending on how much you want to consider sentences as similar. The right value will come up with experimentation, trying to find a value that preserves the similarities between sentences without splitting close sentences into different groups.\n",
    "\n",
    "In general, since we want to end up with very similar sentences in the same cluster, the target should be a value that returns a higher number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_eps = max(n_classes.items(), key=operator.itemgetter(1))[0]\n",
    "\n",
    "dbscan = DBSCAN(eps=max_eps,\n",
    "                min_samples=5,\n",
    "                metric='cosine').fit(x)\n",
    "dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame({'label': dbscan.labels_,\n",
    "                       'sent': sentences\n",
    "                      })\n",
    "result.loc[:, 'vectors'] = [x.tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE is a tool to visualize high-dimensional data. It converts similarities between data points to joint probabilities and tries to minimize the Kullback-Leibler divergence between the joint probabilities of the low-dimensional embedding and the high-dimensional data. t-SNE has a cost function that is not convex, i.e. with different initializations we can get different results.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dimensions to 2\n",
    "tsne_coord = TSNE(n_components=2,\n",
    "             n_jobs=-1).fit_transform(x)\n",
    "\n",
    "x_tsne, y_tsne = [i[0] for i in tsne_coord], [i[1] for i in tsne_coord]\n",
    "\n",
    "result['x'] = x_tsne\n",
    "result['y'] = y_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove un-clustered rows\n",
    "result_filtered = result[result['label'] != -1]\n",
    "result_filtered.shape, result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_filtered.loc[:, 'lable_cat'] = result_filtered.label.astype(str)\n",
    "result_filtered.loc[:, 'lable_cat'] = 'g_' + result_filtered['lable_cat'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_color_map(num_color):\n",
    "\n",
    "    cm = pylab.get_cmap('gist_rainbow')\n",
    "    cgen = (cm(1.*i/num_color) for i in range(num_color))\n",
    "    return [i for i in cgen]\n",
    "\n",
    "c_map = generate_color_map(result_filtered.label.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=[16, 16])\n",
    "sns.scatterplot(x='x', y='y', data=result_filtered,\n",
    "               hue='lable_cat',\n",
    "               sizes  = 30,\n",
    "                palette=c_map\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Transform to Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to arrange those sentences in time and to filter them by relevance. \n",
    "\n",
    "Since there are many titles about the same topic every day, we need a criterium to pick one among them. It should be the sentence that best represents the event, one that comprises the core message which those titles refer to.\n",
    "\n",
    "In order to achieve that, we can group the daily sentences, and for each group (or cluster), choose the one closest to the cluster center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_filtered[result_filtered['label'] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_vector(sents):\n",
    "    a = np.zeros(300)\n",
    "    for sent in sents:\n",
    "        a = a+nlp(sent).vector\n",
    "    return a/len(sents)\n",
    "\n",
    "\n",
    "def get_central_vector(sents):\n",
    "   \n",
    "    sents = sents.reset_index(drop=True)\n",
    "    vecs = []\n",
    "    for sent in sents:\n",
    "        doc = nlp(sent)\n",
    "        vecs.append(doc.vector)\n",
    "        \n",
    "    mean_vec = get_mean_vector(sents)\n",
    "    index = pairwise_distances_argmin_min(np.array([mean_vec]),\n",
    "                                          vecs)[0][0]\n",
    "    return sents[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_central_vector(result_filtered['sent'][result_filtered['label'] == 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "state": {
    "7ab2c0248086497aa5015d1382111118": {
     "views": [
      {
       "cell_index": 9
      }
     ]
    },
    "b8033342c87f45f3b70e3be8cc746e66": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
