{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics import pairwise_distances_argmin_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from newsapi import NewsApiClient\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = 'f392d617107f4dbfa6a672410064e7d1'\n",
    "newsapi = NewsApiClient(api_key=key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_past_articles(past):\n",
    "    past_articles = dict()\n",
    "    for past_days in range(1, past):\n",
    "        from_day = str(dt.now() - timedelta(days=past_days))\n",
    "        to_day = str(dt.now() - timedelta(days=past_days -1))\n",
    "        past_articles.update({from_day:to_day})\n",
    "    return past_articles\n",
    "\n",
    "def get_artciles(query, past=30):\n",
    "    past_articles = get_past_articles(past)\n",
    "    all_articles = []\n",
    "    for i, j in tqdm_notebook(past_articles.items()):\n",
    "        for pag in tqdm_notebook(range(1,6)):\n",
    "            pag_articles = newsapi.get_everything(q=query,\n",
    "                                                 language='en',\n",
    "                                                 from_param=i,\n",
    "                                                 to=j,\n",
    "                                                 sort_by='relevancy',\n",
    "                                                 page=pag)['articles']\n",
    "            if len(pag_articles) == 0: break\n",
    "            all_articles.extend(pag_articles)\n",
    "    return all_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2020-01-04'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-01-05'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_date = dt.strptime(start_date, '%Y-%m-%d') + timedelta(days=1)\n",
    "end_date.strftime(\"%Y-%m-%d\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:16<00:00,  1.84it/s]\n"
     ]
    }
   ],
   "source": [
    "all_articles = []\n",
    "\n",
    "for i in tqdm(range(0,30)):\n",
    "    from_date = dt.strptime(start_date, '%Y-%m-%d') + timedelta(days=i)\n",
    "    end_date = from_date + timedelta(days=1)\n",
    "    \n",
    "    from_date = from_date.strftime(\"%Y-%m-%d\") \n",
    "    end_date = end_date.strftime(\"%Y-%m-%d\") \n",
    "    \n",
    "    for p in range(1, 6):\n",
    "        articles = newsapi.get_everything(\n",
    "                                              sources='bbc-news,the-verge',\n",
    "                                              domains='bbc.co.uk,techcrunch.com',\n",
    "                                              from_param=from_date,\n",
    "                                              to=end_date,\n",
    "                                              language='en',\n",
    "                                              sort_by='relevancy',\n",
    "                                              page=p)['articles']\n",
    "        all_articles.extend(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 8)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_articles = pd.DataFrame.from_dict(all_articles)\n",
    "df_articles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles = df_articles.drop_duplicates(subset='content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles.to_csv('../data/news_api.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/nyt_data_20200202.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pub_date'] = pd.to_datetime(df['pub_date'])\n",
    "\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give meaning to independent words and, consequently, whole sentences, we’ll use SpaCy’s pre-trained word embeddings models. More specifically, SpaCy’s large model (en_core_web_lg), which has pre-trained word vectors for 685k English words. Alternatively, you could be using any pre-trained word representation model (Word2Vec, FastText, GloVe…).\n",
    "\n",
    "By default, SpaCy considers a sentence’s vector as the average between every word’s vector. It’s a simplistic approach that doesn’t take into account the order of words to determine a sentence’s vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vecs = {}\n",
    "docs = []\n",
    "\n",
    "for headline in tqdm_notebook(df['headline']):\n",
    "    doc = nlp(headline)\n",
    "    docs.append(doc)\n",
    "    sent_vecs.update({'headline': doc.vector})\n",
    "\n",
    "sentences = list(sent_vecs.keys())\n",
    "vectors = list(sent_vecs.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The epsilon parameter determines the maximum distance between two samples for them to be considered as in the same neighborhood, meaning that if eps is too big, fewer clusters will be formed, but also if it’s too small, most of the points will be classified as not belonging to a cluster (-1), which will result in a few clusters as well.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(vectors)\n",
    "\n",
    "n_classes = {}\n",
    "\n",
    "for i in tqdm_notebook(np.arange(0.001, 1, 0.001)):\n",
    "    dbscan = DBSCAN(eps=i,\n",
    "                    min_samples=1,\n",
    "                    metric='cosine').fit(x)\n",
    "    n_classes.update({i: len(pd.Series(dbscan.labels_).value_counts())})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(n_classes.keys(), n_classes.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tunning eps value might be one of the most delicate steps because the outcome will vary a lot depending on how much you want to consider sentences as similar. The right value will come up with experimentation, trying to find a value that preserves the similarities between sentences without splitting close sentences into different groups.\n",
    "\n",
    "In general, since we want to end up with very similar sentences in the same cluster, the target should be a value that returns a higher number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.08,\n",
    "                min_samples=2,\n",
    "                metric='cosine').fit(x)\n",
    "dbscan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame({'label': dbscan.labels_,\n",
    "                       'sent': sentences})\n",
    "example_result = result[result['label'] == 1].sent.tolist()\n",
    "event_df = df[df['headline'].isin(example_result)][['pub_date', 'headline']]\n",
    "event_df['pub_date'] = pd.to_datetime(event_df['pub_date'])\n",
    "event_df = event_df.sort_values(by='pub_date').dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Transform to Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to arrange those sentences in time and to filter them by relevance. \n",
    "\n",
    "Since there are many titles about the same topic every day, we need a criterium to pick one among them. It should be the sentence that best represents the event, one that comprises the core message which those titles refer to.\n",
    "\n",
    "In order to achieve that, we can group the daily sentences, and for each group (or cluster), choose the one closest to the cluster center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_vector(sents):\n",
    "    a = np.zeros(300)\n",
    "    for sent in sents:\n",
    "        a = a+nlp(sent).vector\n",
    "    return a/len(sents)\n",
    "\n",
    "\n",
    "def get_central_vector(sents):\n",
    "    vecs = []\n",
    "    for sent in sents:\n",
    "        doc = nlp(title)\n",
    "        vecs.append(doc.vector)\n",
    "    mean_vec = get_mean_vector(sents)\n",
    "    index = pairwise_distances_argmin_min(np.array([mean_vec]),\n",
    "                                          vecs)[0][0]\n",
    "    return sents[index]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {
    "7beda84ce5c84c4c8be053e5d3070c5b": {
     "views": [
      {
       "cell_index": 11
      }
     ]
    },
    "c3278cce2c684f2ca8b7380ae972c3cc": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
